{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1Ru5mRlca6aWbw5GwG2t0wAqtR5u_8BqW",
      "authorship_tag": "ABX9TyNSEx2t53VNh+Erv4KvVzdj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivorima/CBIR-NearDuplicateDetection/blob/main/Near_Duplicates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKXt6moSzl89"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from scipy.spatial.distance import cosine\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing the images to compare\n",
        "images_directory = '/content/drive/MyDrive/test_dataset'\n",
        "# \"..\\corpus_lipade\\presse\\photos\\jpg\""
      ],
      "metadata": {
        "id": "A7OeM0L3JWY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24144fa1-819d-443e-a025-4630dd4da515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    input_image = Image.open(image_path)\n",
        "    preprocess = transforms.Compose([\n",
        "        # Convert the image to grayscale\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        # Since you've changed your images to grayscale (1-channel), you also need to modify the first layer of the model to accept 1-channel input. However, remember that this will invalidate the pretrained weights for this layer. Here's how you can modify the first layer:\n",
        "\n",
        "        # this is how images are preprocessed when trained on Imagenet\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    input_tensor = preprocess(input_image)\n",
        "    input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n",
        "    return input_batch\n",
        "\n",
        "def get_features(image_batch, model):\n",
        "    if torch.cuda.is_available():\n",
        "        image_batch = image_batch.to('cuda')\n",
        "    with torch.no_grad():\n",
        "        features = model(image_batch)\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "\n",
        "def compare_images(feature1, feature2, threshold=0.85):\n",
        "    similarity = 1 - cosine(feature1.flatten(), feature2.flatten())\n",
        "    return similarity > threshold"
      ],
      "metadata": {
        "id": "yhoufY35H8tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the pre-trained ResNet model"
      ],
      "metadata": {
        "id": "2dFyI53rj5-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
        "\n",
        "# # Modify the first convolutional layer\n",
        "# # Original first layer: 3 input channels, 64 output channels, kernel size 7, stride 2, padding 3, bias=False\n",
        "# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Remove the last classification layer\n",
        "model = torch.nn.Sequential(*(list(model.children())[:-1]))  # Remove the last classification layer\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')"
      ],
      "metadata": {
        "id": "xMX3Y0PdID8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c90027-7a67-44be-e957-bda18503981e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting features of all images"
      ],
      "metadata": {
        "id": "acZ-VuyfjE6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = {}\n",
        "for image_name in os.listdir(images_directory):\n",
        "    if image_name.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(images_directory, image_name)\n",
        "        image_batch = preprocess_image(image_path)\n",
        "        features = get_features(image_batch, model)\n",
        "        image_features[image_name] = features"
      ],
      "metadata": {
        "id": "Cj_h3qMei-YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare each image to every other image"
      ],
      "metadata": {
        "id": "OwEKcqadjN1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_images = {}\n",
        "for img1, features1 in image_features.items():\n",
        "    similar_images[img1] = []\n",
        "    for img2, features2 in image_features.items():\n",
        "        if img1 != img2 and compare_images(features1, features2):\n",
        "            similar_images[img1].append(img2)"
      ],
      "metadata": {
        "id": "BsLrYIBZINvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert the results to a DataFrame for easy export to Excel"
      ],
      "metadata": {
        "id": "G5Ag3SvXjpNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([(k, v) for k, v in similar_images.items()], columns=['Reference Image', 'Similar Images'])\n",
        "df['Similar Images'] = df['Similar Images'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "# Save to Excel file\n",
        "df.to_excel('/content/drive/MyDrive/similar_images.xlsx', index=False)"
      ],
      "metadata": {
        "id": "xCgyH8g_jRBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formatting the data"
      ],
      "metadata": {
        "id": "Bdu9IJiAYkyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the ground truth dataset\n",
        "file_path_ground_truth = '/content/drive/MyDrive/test_dataset/donnees_IS_new.xlsx'  # Replace with your file path\n",
        "ground_truth_df = pd.read_excel(file_path_ground_truth)\n",
        "\n",
        "# Set the first column as the reference image\n",
        "ground_truth_df['Reference Image'] = ground_truth_df.iloc[:, 0]\n",
        "\n",
        "# Consolidate other columns into one\n",
        "ground_truth_df['Similar Images'] = ground_truth_df.iloc[:, 1:].apply(\n",
        "    lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Create a new DataFrame with the desired structure\n",
        "reshaped_ground_truth_df = ground_truth_df[['Reference Image', 'Similar Images']]\n",
        "\n",
        "# Optionally, save the reshaped DataFrame to a new Excel file\n",
        "reshaped_ground_truth_df.to_excel('/content/drive/MyDrive/reshaped_ground_truth.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "CYaT-WlpYPNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(ground_truth, predictions):\n",
        "\n",
        "    # Initialize counters\n",
        "    tp, fp, fn = 0, 0, 0\n",
        "\n",
        "    # Convert similar images in ground truth to a set for efficient lookup\n",
        "    ground_truth_dict = ground_truth.set_index('Reference Image')['Similar Images'].to_dict()\n",
        "    ground_truth_dict = {k: set(str(v).split(', ')) for k, v in ground_truth_dict.items()}\n",
        "\n",
        "    # Iterate through each reference image in the predictions\n",
        "    for index, row in predictions.iterrows():\n",
        "        ref_image = row['Reference Image']\n",
        "        predicted_similar = set(str(row['Similar Images']).split(', '))\n",
        "\n",
        "        # Get the corresponding ground truth similar images\n",
        "        actual_similar = ground_truth_dict.get(ref_image, set())\n",
        "\n",
        "        # Calculate TP, FP, and FN\n",
        "        tp += len(predicted_similar.intersection(actual_similar))\n",
        "        fp += len(predicted_similar - actual_similar)\n",
        "        fn += len(actual_similar - predicted_similar)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "UvirzamlYQHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "file_path_ground_truth = '/content/drive/MyDrive/reshaped_ground_truth.xlsx'  # Replace with your file path\n",
        "file_path_predictions = '/content/drive/MyDrive/similar_images.xlsx'  # Replace with your file path\n",
        "\n",
        "ground_truth_df = pd.read_excel(file_path_ground_truth)\n",
        "predictions_df = pd.read_excel(file_path_predictions)\n",
        "\n",
        "# Calculate the metrics\n",
        "precision, recall, f1 = calculate_metrics(ground_truth_df, predictions_df)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6Jn-8n-Zfn2",
        "outputId": "68dde097-31ec-437d-a028-de365539d02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.048879837067209775\n",
            "Recall: 0.35294117647058826\n",
            "F1 Score: 0.08586762075134169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RESNET 101**\n",
        "# Threshold: 0.9\n",
        "* Precision: 0.05851063829787234\n",
        "* Recall: 0.16176470588235295\n",
        "* F1 Score: 0.0859375\n",
        "\n",
        "\n",
        "# Threshold: 0.89\n",
        "* Precision: 0.05963302752293578\n",
        "* Recall: 0.19117647058823528\n",
        "* F1 Score: 0.09090909090909093\n",
        "\n",
        "# Threshold: 0.88\n",
        "* Precision: **0.061567164179104475**\n",
        "* Recall: 0.2426470588235294\n",
        "* F1 Score: **0.0982142857142857**\n",
        "\n",
        "# Threshold: 0.87\n",
        "* Precision: 0.056338028169014086\n",
        "* Recall: 0.27941176470588236\n",
        "* F1 Score: 0.09376927822331894\n",
        "\n",
        "# Threshold: 0.85\n",
        "* Precision: 0.043254817987152035\n",
        "* Recall: **0.3713235294117647**\n",
        "* F1 Score: 0.0774836977368623\n",
        "\n",
        "\n",
        "## **RESNET 152**\n",
        "# Threshold: 0.88\n",
        "* Precision: 0.06626506024096386\n",
        "* Recall: 0.2426470588235294\n",
        "* F1 Score: 0.10410094637223975\n",
        "\n",
        "# Threshold: 0.85\n",
        "* Precision: 0.048879837067209775\n",
        "* Recall: 0.35294117647058826\n",
        "* F1 Score: 0.08586762075134169"
      ],
      "metadata": {
        "id": "DetRb0_WWx_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "* trier et prendre les '10' plus similaires\n",
        "* pas de threshold unique\n",
        "* augmentations, transformations aléatoires  ? yes\n",
        "lesquelles ?\n",
        "* fine tuning sur les classes, evaluer sur une partie du test dataset (mini testset)\n",
        "(tache classification, use cross entropy classifica)\n",
        "* captions ? not for resnet\n",
        "* début janvier\n",
        "* score ndcg pour prendre en compte le classement\n",
        "* tesni ??? colorier les classes pour la présentation"
      ],
      "metadata": {
        "id": "4quWafIimj98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* faire un tri et couper chaque 10 a 9 image\n",
        "* la classe qui a le plus d'image pour le fait de couper\n",
        "* Protocole d'évaluation précis\n",
        "* changer bert par un autre\n",
        "* SNI\n",
        "* ndcg la position des images sur au lieu du average precision"
      ],
      "metadata": {
        "id": "4A-XI-JL9ow3"
      }
    }
  ]
}