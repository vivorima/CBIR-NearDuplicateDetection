{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1Ru5mRlca6aWbw5GwG2t0wAqtR5u_8BqW",
      "authorship_tag": "ABX9TyNQ+KIlphiaYup99Ag9LngS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKXt6moSzl89"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from scipy.spatial.distance import cosine\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing our images\n",
        "images_directory = '/content/drive/MyDrive/test_dataset'\n",
        "# \"..\\corpus_lipade\\presse\\photos\\jpg\""
      ],
      "metadata": {
        "id": "A7OeM0L3JWY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24144fa1-819d-443e-a025-4630dd4da515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing the Data to fit ResNet\n",
        "\n"
      ],
      "metadata": {
        "id": "aj2XvU1XmE5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    input_image = Image.open(image_path)\n",
        "    preprocess = transforms.Compose([\n",
        "        # Convert the image to grayscale but keeping all 3 channels\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    input_tensor = preprocess(input_image)\n",
        "    input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n",
        "    return input_batch\n",
        "\n",
        "def get_features(image_batch, model):\n",
        "    if torch.cuda.is_available():\n",
        "        image_batch = image_batch.to('cuda')\n",
        "    with torch.no_grad():\n",
        "        features = model(image_batch)\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "\n",
        "def compare_images(feature1, feature2, threshold):\n",
        "    similarity = 1 - cosine(feature1.flatten(), feature2.flatten())\n",
        "    return similarity > threshold"
      ],
      "metadata": {
        "id": "yhoufY35H8tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the pre-trained ResNet model"
      ],
      "metadata": {
        "id": "2dFyI53rj5-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
        "\n",
        "# Removing the last classification layer\n",
        "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')"
      ],
      "metadata": {
        "id": "xMX3Y0PdID8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c90027-7a67-44be-e957-bda18503981e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting features of all images"
      ],
      "metadata": {
        "id": "acZ-VuyfjE6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = {}\n",
        "for image_name in os.listdir(images_directory):\n",
        "    if image_name.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(images_directory, image_name)\n",
        "        image_batch = preprocess_image(image_path)\n",
        "        features = get_features(image_batch, model)\n",
        "        image_features[image_name] = features"
      ],
      "metadata": {
        "id": "Cj_h3qMei-YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing each image to every other image"
      ],
      "metadata": {
        "id": "OwEKcqadjN1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Threshold = 0.88\n",
        "similar_images = {}\n",
        "for img1, features1 in image_features.items():\n",
        "    similar_images[img1] = []\n",
        "    for img2, features2 in image_features.items():\n",
        "        if img1 != img2 and compare_images(features1, features2,Threshold):\n",
        "            similar_images[img1].append(img2)"
      ],
      "metadata": {
        "id": "BsLrYIBZINvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(ground_truth, predictions):\n",
        "\n",
        "    # Initializing\n",
        "    tp, fp, fn = 0, 0, 0\n",
        "\n",
        "    # Converting ground truth to a dict\n",
        "    ground_truth_dict = ground_truth.set_index('Reference Image')['Similar Images'].to_dict()\n",
        "    ground_truth_dict = {k: set(str(v).split(', ')) for k, v in ground_truth_dict.items()}\n",
        "\n",
        "    # for each reference image in the predictions\n",
        "    for index, row in predictions.iterrows():\n",
        "        ref_image = row['Reference Image']\n",
        "        predicted_similar = set(str(row['Similar Images']).split(', '))\n",
        "\n",
        "        # Get the corresponding ground truth\n",
        "        actual_similar = ground_truth_dict.get(ref_image, set())\n",
        "\n",
        "        # Calculate TP, FP, and FN\n",
        "        tp += len(predicted_similar.intersection(actual_similar))\n",
        "        fp += len(predicted_similar - actual_similar)\n",
        "        fn += len(actual_similar - predicted_similar)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "UvirzamlYQHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "file_path_ground_truth = '/content/drive/MyDrive/ground_truth.xlsx'\n",
        "file_path_predictions = '/content/drive/MyDrive/similar_images.xlsx'\n",
        "ground_truth_df = pd.read_excel(file_path_ground_truth)\n",
        "predictions_df = pd.read_excel(file_path_predictions)\n",
        "\n",
        "# Calculate the metrics\n",
        "precision, recall, f1 = calculate_metrics(ground_truth_df, predictions_df)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6Jn-8n-Zfn2",
        "outputId": "68dde097-31ec-437d-a028-de365539d02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.048879837067209775\n",
            "Recall: 0.35294117647058826\n",
            "F1 Score: 0.08586762075134169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Historique des resultats\n",
        "\n",
        "## **RESNET 101**\n",
        "# Threshold: 0.9\n",
        "* Precision: 0.05851063829787234\n",
        "* Recall: 0.16176470588235295\n",
        "* F1 Score: 0.0859375\n",
        "\n",
        "\n",
        "# Threshold: 0.89\n",
        "* Precision: 0.05963302752293578\n",
        "* Recall: 0.19117647058823528\n",
        "* F1 Score: 0.09090909090909093\n",
        "\n",
        "# Threshold: 0.88\n",
        "* Precision: **0.061567164179104475**\n",
        "* Recall: 0.2426470588235294\n",
        "* F1 Score: **0.0982142857142857**\n",
        "\n",
        "# Threshold: 0.87\n",
        "* Precision: 0.056338028169014086\n",
        "* Recall: 0.27941176470588236\n",
        "* F1 Score: 0.09376927822331894\n",
        "\n",
        "# Threshold: 0.85\n",
        "* Precision: 0.043254817987152035\n",
        "* Recall: **0.3713235294117647**\n",
        "* F1 Score: 0.0774836977368623\n",
        "\n",
        "\n",
        "## **RESNET 152**\n",
        "# Threshold: 0.88\n",
        "* Precision: 0.06626506024096386\n",
        "* Recall: 0.2426470588235294\n",
        "* F1 Score: 0.10410094637223975\n",
        "\n",
        "# Threshold: 0.85\n",
        "* Precision: 0.048879837067209775\n",
        "* Recall: 0.35294117647058826\n",
        "* F1 Score: 0.08586762075134169"
      ],
      "metadata": {
        "id": "DetRb0_WWx_J"
      }
    }
  ]
}